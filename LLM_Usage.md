# LLM_Usage.md

## 1. Overview
During the development of the VVAP – Gaming Laptop Review Aggregator, the team used Large Language Models (LLMs) as a development assistant to support coding, data synthesis, documentation, and debugging. LLMs were not part of the production system; instead, they accelerated backend, frontend, and mock-data creation workflows.

## 2. LLM Usage Scenarios

### 2.1. Code Generation
LLMs were used to generate and refine several code components:

- Backend (Express + MySQL):
  - Drafting REST API route structures for `/reviews`, `/aggregate`, and `/fetch`.
  - Suggesting controller/service architecture and mock data logic.
  - Creating initial SQL schema, composite keys, and seed script.

- Frontend (React + Vite):
  - Generating React component templates for charts, tables, and cards.
  - Creating API fetch functions with proper error handling.
  - Drafting UI logic for filtering and state management.

- DevOps:
  - Drafting `docker-compose.yml` and `.env` configuration examples.

**Example prompt:**  
"Generate an Express controller that aggregates ratings by source and returns average, count, min, and max."

### 2.2. Data Synthesis (Mock Data)
LLMs generated synthetic review data used during testing:

- Created realistic sample reviews for Amazon, BestBuy, and Newegg.
- Generated varied ratings (1–5 stars) and author/title/body text.
- Ensured datasets were large enough to test API performance and charts.

### 2.3. Documentation Support
LLMs supported the creation of clear, concise documentation:

- Project README structure and technical descriptions.
- API descriptions and data model explanations.
- Clarification of architecture, workflow, and developer instructions.

This allowed the team to focus more on implementing the core features.

## 3. Verification & Validation

### 3.1. Code Verification
All LLM-generated code was manually reviewed:
- Run locally in dev mode to ensure no syntax or logical errors.
- Checked against existing project structure and dependencies.
- Adjusted to match naming conventions and database schema.

### 3.2. Data Verification
Mock data generated by the LLM was checked for:
- Valid rating range (1–5).
- Unique review IDs per source.
- Balanced distribution for meaningful chart results.

### 3.3. Prompt Refinement
To improve LLM output, the team used:
- Iterative prompting (step-by-step improvements).
- Error-driven prompts (pasting logs to diagnose issues).
- Strict formatting constraints (e.g., "return only JSON").

## 4. Challenges When Using LLM

### 4.1. Hallucinated Code
LLM occasionally produced:
- Invalid imports
- Non-existent functions
- Incorrect Express or React patterns

These required manual correction.

### 4.2. SQL-Related Issues
Common LLM mistakes:
- Incorrect JOINs
- Missing composite primary keys
- Queries not matching MySQL syntax

Team members needed to fix these before integration.

### 4.3. Synthetic Data Limitations
Generated reviews were sometimes repetitive or unrealistic, requiring multiple prompt iterations to improve diversity.

### 4.4. Risk of Overreliance
LLM suggestions could not replace real architectural thinking. The team validated every critical decision regarding API design, DB structure, and component architecture.

## 5. Conclusion
LLMs significantly improved development speed by assisting in code generation, documentation, and mock data creation. However, every LLM-generated output was reviewed, corrected, and verified by the team before use. LLM served as a helpful tool, not an automated decision maker.
